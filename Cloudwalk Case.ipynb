{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1590dd3",
   "metadata": {},
   "source": [
    "# Dark Souls 3: Knowledge Graph Construction\n",
    "\n",
    "This notebook presents a step-by-step process for constructing a knowledge graph from in-game textual data extracted from *Dark Souls III*.\n",
    "\n",
    "Dark Souls is known for its indirect, cryptic narrative style. Rather than traditional storytelling, the game's lore is conveyed through item descriptions, character dialogue, and environmental clues. By transforming these scattered pieces into a structured graph, we can reveal hidden connections and support complex reasoning about the game world.\n",
    "\n",
    "The dataset used here was extracted from the game files and made available by the [DarkSouls3.TextViewer project](https://github.com/mrexodia/DarkSouls3.TextViewer), which aggregates in-game texts such as item descriptions and NPC dialogue.\n",
    "\n",
    "This notebook includes:\n",
    "- Data extraction and preparation from the JSON dump\n",
    "- Entity and relationship extraction using language models\n",
    "- Knowledge graph construction using the extracted triples\n",
    "- Graph exploration and insight generation\n",
    "- Graph-based question answering via queries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c550e55",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d807ff51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import GPT2TokenizerFast\n",
    "import random\n",
    "import time\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import networkx as nx\n",
    "from neo4j import GraphDatabase\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9fa596",
   "metadata": {},
   "source": [
    "## 2. Loading the Original JSON Dataset\n",
    "\n",
    "The full dataset is provided as a JSON file (`ds3.json`) containing raw textual content extracted directly from the game files of *Dark Souls III*, including item descriptions, spell texts, equipment lore, and NPC dialogues.\n",
    "\n",
    "To focus the analysis, I filter the dataset to retain only the entries related to **accessories**, **magic**, **weapons**, and **conversations** — categories that typically contain the richest lore content and entity interactions. This results in a more concise and meaningful subset for relationship extraction.\n",
    "\n",
    "The filtered texts are saved as `ds3_clean_texts.json`, which serves as the foundation for the knowledge extraction and graph construction steps that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33ebfee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ds3.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "eng_data = data[\"languages\"][\"engUS\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbee0c3f",
   "metadata": {},
   "source": [
    "## 3. Selecting Lore-Rich Categories\n",
    "\n",
    "To ensure the graph captures meaningful narrative relationships, I restrict the scope to the following categories:\n",
    "\n",
    "- **accessory**\n",
    "- **magic**\n",
    "- **weapon**\n",
    "- **conversations**\n",
    "\n",
    "These groups are known to contain dense lore elements and references to entities and events within the Dark Souls universe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f48d0c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_to_use = [\"accessory\", \"magic\", \"weapon\", \"conversations\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5752618e",
   "metadata": {},
   "source": [
    "## 4. Extracting and Structuring Text Entries\n",
    "\n",
    "For each entry in the selected categories, I extract the `name`, `description`, and `knowledge` fields when available. These components contain the textual content that later serves as input for relationship extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71a6b697",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = []\n",
    "\n",
    "for category in categories_to_use:\n",
    "    for entry in eng_data[category].values():\n",
    "        name = entry.get(\"name\", \"\").strip()\n",
    "        description = entry.get(\"description\", \"\").strip()\n",
    "        knowledge = entry.get(\"knowledge\", \"\").strip()\n",
    "        if name or description or knowledge:\n",
    "            clean_data.append({\n",
    "                \"category\": category,\n",
    "                \"name\": name,\n",
    "                \"description\": description,\n",
    "                \"knowledge\": knowledge\n",
    "            })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76556a0a",
   "metadata": {},
   "source": [
    "## 5. Saving the Cleaned Dataset\n",
    "\n",
    "To streamline development and avoid reprocessing the full JSON file each time, I save the filtered and structured entries as `ds3_clean_texts.json`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1803919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted and saved: 3473 entries in ds3_clean_texts.json\n"
     ]
    }
   ],
   "source": [
    "with open(\"ds3_clean_texts.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(clean_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ Extracted and saved: {len(clean_data)} entries in ds3_clean_texts.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645250e6",
   "metadata": {},
   "source": [
    "## 6. Inspecting the Cleaned Dataset\n",
    "\n",
    "I load the cleaned dataset and display a few samples to verify encoding, formatting, and overall relevance before proceeding to language model extraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8baf5e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entries: 3473\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'category': 'accessory',\n",
       "  'name': \"Havel's Ring\",\n",
       "  'description': 'Boosts maximum equipment load',\n",
       "  'knowledge': \"This ring was named after Havel the Rock,\\nLord Gwyn's old battlefield compatriot.\\n\\nHavel's men wore the ring to express faith in\\ntheir leader and to carry a heavier load.\"},\n",
       " {'category': 'accessory',\n",
       "  'name': 'Red Tearstone Ring',\n",
       "  'description': 'Boosts ATK while HP is low',\n",
       "  'knowledge': 'The rare gem called tearstone has the\\nuncanny ability to sense imminent death.\\n\\nThis red tearstone from Carim boosts the\\nattack of its wearer when in danger.'},\n",
       " {'category': 'accessory',\n",
       "  'name': 'Darkmoon Blade Covenant Ring',\n",
       "  'description': \"Answer Dark Sun Gwyndolin's summoning\",\n",
       "  'knowledge': \"Ring granted to those bound by the\\nDarkmoon Blade covenant.\\nAnswer Dark Sun Gwyndolin's summoning.\\n\\nGwyndolin, all too aware of his repulsive,\\nfrail appearance, created the illusion of \\na sister Gwynevere, who helps him guard\\nover Anor Londo. An unmasking of these\\ndeities would be tantamount to blasphemy.\"}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"ds3_clean_texts.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    clean_data = json.load(f)\n",
    "\n",
    "print(f\"Total entries: {len(clean_data)}\")\n",
    "clean_data[:3]  # inspect first few items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3d3231",
   "metadata": {},
   "source": [
    "## 7. Token Estimation and Sampling Strategy\n",
    "\n",
    "Before running large-scale extraction with GPT-4o, I estimate the token usage to control costs and processing time. The full dataset contains approximately 268,000 tokens. Based on GPT-4o pricing — $0.005 per 1,000 input tokens and $0.015 per 1,000 output tokens — the total cost would be around $2.\n",
    "\n",
    "To strike a balance between coverage, cost, and iteration speed, I select a representative sample of 100 entries for initial experimentation. This approach keeps the total cost under $1 while preserving the diversity of item types in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74da807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated total tokens (input only): 268,105\n"
     ]
    }
   ],
   "source": [
    "# Load GPT2 tokenizer\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "def count_tokens(texts):\n",
    "    total_tokens = 0\n",
    "    for entry in clean_data:\n",
    "        combined = f\"{entry['name']}\\n{entry['description']}\\n{entry['knowledge']}\"\n",
    "        tokens = len(tokenizer.encode(combined))\n",
    "        total_tokens += tokens\n",
    "    return total_tokens\n",
    "\n",
    "total_tokens = count_tokens(clean_data)\n",
    "print(f\"Estimated total tokens (input only): {total_tokens:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0948e6af",
   "metadata": {},
   "source": [
    "## 8. Sampling Entries for Triple Extraction\n",
    "\n",
    "I randomly sample 100 entries from the cleaned dataset, ensuring a variety of item types (e.g., weapons, armor, spells, NPC dialogue) are included.\n",
    "\n",
    "These entries are saved as `sampled_lore_entries.json` and serve as input for the GPT-4o-based triple extraction in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1ccff8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Sample 100 entries from the cleaned dataset\n",
    "sampled_data = random.sample(clean_data, 100)\n",
    "\n",
    "# Save to file for processing with the OpenAI API\n",
    "with open(\"sampled_lore_entries.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(sampled_data, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839f5dcc",
   "metadata": {},
   "source": [
    "### 9. Extracting Triples Using GPT-4o\n",
    "\n",
    "To transform the item descriptions into structured knowledge, I use **GPT-4o** via the OpenAI API.\n",
    "\n",
    "For each entry — containing a `name`, `description`, and optional `knowledge` field — a structured prompt is sent to the model, asking it to extract **1 to 3 subject–predicate–object triples** that represent meaningful relationships between entities.\n",
    "\n",
    "Each extracted triple follows the format:\n",
    "\n",
    "(\"Entity A\", \"relation\", \"Entity B\")\n",
    "\n",
    "This step automates the conversion of unstructured game lore into structured data, making it suitable for graph-based analysis and visualization.\n",
    "\n",
    "The extracted triples are saved to `extracted_triples.json` and will be used in the next step to build the knowledge graph.\n",
    "\n",
    "> ℹ️ **Note:** To re-run this step, you must provide a valid OpenAI API key. The notebook is configured to read the key from an environment variable (`OPENAI_API_KEY`) loaded via a `.env` file.\n",
    "\n",
    "However, this step has already been executed, and the output file `extracted_triples.json` is included in the repository. There’s no need to re-run the extraction unless you wish to modify the prompt or input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8a9ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'gpt-4o' is available. Proceeding with extraction.\n",
      "Loaded 100 entries for triple extraction.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting triples: 100%|██████████| 100/100 [04:12<00:00,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction completed. Results saved to 'extracted_triples.json'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize OpenAI client\n",
    "load_dotenv()\n",
    "client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# 2. Choose the model\n",
    "MODEL = \"gpt-4o\"\n",
    "\n",
    "# 3. Verify that the model exists in the account\n",
    "available_models = [m.id for m in client.models.list().data]\n",
    "if MODEL not in available_models:\n",
    "    raise ValueError(f\"Model '{MODEL}' not available in your account. Available models are:\\n{available_models}\")\n",
    "\n",
    "print(f\"Model '{MODEL}' is available. Proceeding with extraction.\")\n",
    "\n",
    "# 4. Load sampled entries\n",
    "with open(\"sampled_lore_entries.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    entries = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(entries)} entries for triple extraction.\")\n",
    "\n",
    "# 5. Define the system prompt\n",
    "system_prompt = (\n",
    "    \"You are a helpful assistant that extracts meaningful relationships from fantasy game item descriptions.\"\n",
    ")\n",
    "\n",
    "# 6. Function to generate user prompts\n",
    "def make_user_prompt(entry):\n",
    "    content = f\"{entry['name']}\\n{entry['description']}\\n{entry['knowledge']}\".strip()\n",
    "    return (\n",
    "        f\"\"\"Given the following game lore text, extract 1 to 3 subject–predicate–object triples.\n",
    "Each triple should describe a meaningful relationship between entities mentioned in the text.\n",
    "\n",
    "Return your answer strictly as a Python list of triples like this:\n",
    "[\n",
    "  (\"Entity A\", \"relation\", \"Entity B\"),\n",
    "  ...\n",
    "]\n",
    "\n",
    "Text:\n",
    "\\\"\\\"\\\"\n",
    "{content}\n",
    "\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "    )\n",
    "\n",
    "# 7. Initialize results list\n",
    "results = []\n",
    "\n",
    "# 8. Loop through entries and call OpenAI API\n",
    "for entry in tqdm(entries, desc=\"Extracting triples\"):\n",
    "    try:\n",
    "        user_prompt = make_user_prompt(entry)\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.2\n",
    "        )\n",
    "\n",
    "        output = response.choices[0].message.content.strip()\n",
    "\n",
    "        results.append({\n",
    "            \"input\": entry,\n",
    "            \"triples\": output,\n",
    "            \"error\": None\n",
    "        })\n",
    "\n",
    "        time.sleep(1)  # Optional delay to avoid rate limits\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error on entry '{entry.get('name', 'N/A')}': {e}\")\n",
    "        results.append({\n",
    "            \"input\": entry,\n",
    "            \"triples\": None,\n",
    "            \"error\": str(e)\n",
    "        })\n",
    "\n",
    "# 9. Save results to JSON\n",
    "with open(\"extracted_triples.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Extraction completed. Results saved to 'extracted_triples.json'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8b97c8",
   "metadata": {},
   "source": [
    "### 10. Inspecting Extracted Triples\n",
    "\n",
    "Before proceeding to graph construction, I inspect a random sample of the extracted triples to verify their **quality and consistency**.\n",
    "\n",
    "This step ensures that GPT-4o correctly followed the prompt structure, extracted **meaningful subject–predicate–object relationships**, and returned **syntactically valid outputs**.\n",
    "\n",
    "By reviewing a few examples, I can check for common issues such as:\n",
    "\n",
    "- Formatting errors  \n",
    "- Hallucinated or irrelevant entities  \n",
    "- Incomplete or malformed triples  \n",
    "\n",
    "This verification step helps determine whether any cleaning or adjustments are needed before building the graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9268d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1\n",
      "--------------------------------------------------------------------------------\n",
      "Item Name: Blood Red and White Shield\n",
      "Description: \n",
      "Knowledge: Standard round wooden shield. It features a striking red and white design.\n",
      "\n",
      "Wooden shields are light, manageable, and offer relatively high magic absorption.\n",
      "\n",
      "Skill: Parry\n",
      "Repel an attack at the right time to follow up with a critical hit. Works while equipped in either hand.\n",
      "\n",
      "Extracted Triples:\n",
      "```python\n",
      "[\n",
      "  (\"Blood Red and White Shield\", \"features\", \"red and white design\"),\n",
      "  (\"Wooden shields\", \"offer\", \"high magic absorption\"),\n",
      "  (\"Skill: Parry\", \"allows\", \"repelling an attack to follow up with a critical hit\")\n",
      "]\n",
      "```\n",
      "\n",
      "No errors.\n",
      "================================================================================\n",
      "\n",
      "Sample 2\n",
      "--------------------------------------------------------------------------------\n",
      "Item Name: Fire Crest Shield\n",
      "Description: \n",
      "Knowledge: Shield of the Knights of Blue, engraved with a crest.\n",
      "\n",
      "One of the enchanted blue shields. The Crest\n",
      "Shield in particular greatly reduces magic damage.\n",
      "\n",
      "Skill: Parry\n",
      "Effective regardless of which hand\n",
      "the shield is equipped in. Repel an\n",
      "attack with graceful timing and\n",
      "follow up with a critical hit.\n",
      "\n",
      "Extracted Triples:\n",
      "```python\n",
      "[\n",
      "  (\"Fire Crest Shield\", \"engraved with\", \"crest\"),\n",
      "  (\"Crest Shield\", \"greatly reduces\", \"magic damage\"),\n",
      "  (\"Parry\", \"is skill of\", \"Fire Crest Shield\")\n",
      "]\n",
      "```\n",
      "\n",
      "No errors.\n",
      "================================================================================\n",
      "\n",
      "Sample 3\n",
      "--------------------------------------------------------------------------------\n",
      "Item Name: Poison Uchigatana\n",
      "Description: \n",
      "Knowledge: A unique katana characterized by the fine craftsmanship of an eastern land where it was forged.\n",
      "\n",
      "The finely-sharpened blade cuts flesh like butter and causes bleeding, but breaks easily as a result.\n",
      "\n",
      "Skill: Hold\n",
      "Assume a holding stance to rapidly execute a lunging slash with normal attack, or a deflecting parry with strong attack.\n",
      "\n",
      "Extracted Triples:\n",
      "```python\n",
      "[\n",
      "  (\"Poison Uchigatana\", \"characterized by\", \"fine craftsmanship of an eastern land\"),\n",
      "  (\"finely-sharpened blade\", \"causes\", \"bleeding\"),\n",
      "  (\"finely-sharpened blade\", \"breaks\", \"easily\")\n",
      "]\n",
      "```\n",
      "\n",
      "No errors.\n",
      "================================================================================\n",
      "\n",
      "Sample 4\n",
      "--------------------------------------------------------------------------------\n",
      "Item Name: Dark Bastard Sword\n",
      "Description: \n",
      "Knowledge: A widely-used heavy greatsword normally wielded with two hands.\n",
      "\n",
      "Broad horizontal sweeping attacks make this sword effective against multiple enemies, but unwieldy in narrow spaces.\n",
      "\n",
      "Skill: Stomp\n",
      "Use one's weight to lunge forward with a low stance and increased poise, and follow with strong attack for an upward slash.\n",
      "\n",
      "Extracted Triples:\n",
      "```python\n",
      "[\n",
      "  (\"Dark Bastard Sword\", \"is\", \"heavy greatsword\"),\n",
      "  (\"Dark Bastard Sword\", \"has skill\", \"Stomp\"),\n",
      "  (\"Stomp\", \"allows\", \"lunge forward with a low stance and increased poise\")\n",
      "]\n",
      "```\n",
      "\n",
      "No errors.\n",
      "================================================================================\n",
      "\n",
      "Sample 5\n",
      "--------------------------------------------------------------------------------\n",
      "Item Name: Lightning Eagle Kite Shield\n",
      "Description: \n",
      "Knowledge: Orthodox metal shield engraved with a crest depicting a silver eagle.\n",
      "\n",
      "Medium shields are the most average of shields, providing a practical balance of damage absorption, stability and weight.\n",
      "\n",
      "Skill: Weapon Skill\n",
      "Equipping this shield in the left hand allows one to perform the Skill of the right hand weapon.\n",
      "\n",
      "Extracted Triples:\n",
      "```python\n",
      "[\n",
      "  (\"Lightning Eagle Kite Shield\", \"engraved with\", \"crest depicting a silver eagle\"),\n",
      "  (\"Medium shields\", \"provide\", \"a practical balance of damage absorption, stability and weight\"),\n",
      "  (\"Equipping this shield in the left hand\", \"allows\", \"one to perform the Skill of the right hand weapon\")\n",
      "]\n",
      "```\n",
      "\n",
      "No errors.\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the extracted triples\n",
    "with open(\"extracted_triples.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    extracted = json.load(f)\n",
    "\n",
    "# Sample 5 random entries\n",
    "sample = random.sample(extracted, 5)\n",
    "\n",
    "# Display the results\n",
    "for i, entry in enumerate(sample, 1):\n",
    "    print(f\"Sample {i}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Item Name: {entry['input'].get('name', 'N/A')}\")\n",
    "    print(f\"Description: {entry['input'].get('description', '').strip()}\")\n",
    "    print(f\"Knowledge: {entry['input'].get('knowledge', '').strip()}\")\n",
    "    print(\"\\nExtracted Triples:\")\n",
    "    print(entry.get('triples', 'No triples extracted'))\n",
    "    print(\"\\nError:\" if entry.get('error') else \"\\nNo errors.\")\n",
    "    if entry.get('error'):\n",
    "        print(entry.get('error'))\n",
    "    print(\"=\" * 80)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3d9814",
   "metadata": {},
   "source": [
    "### 11. Parsing and Cleaning Extracted Triples\n",
    "\n",
    "Before constructing the knowledge graph, I parse the extracted triples and apply light cleaning to improve **entity consistency**.\n",
    "\n",
    "The cleaning process includes:\n",
    "\n",
    "- Removing generic prefixes like `\"Skill:\"` or `\"Skill\"` from entity names  \n",
    "- Simplifying overly verbose subjects and objects when possible  \n",
    "- Trimming whitespace and normalizing text formats  \n",
    "\n",
    "These steps help reduce duplication and enhance the quality of nodes and edges in the final graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab8920ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed and cleaned 300 triples.\n"
     ]
    }
   ],
   "source": [
    "# Clean the entity names\n",
    "def clean_entity(text):\n",
    "    text = text.strip()\n",
    "    if text.lower().startswith(\"skill:\"):\n",
    "        text = text[len(\"skill:\"):].strip()\n",
    "    elif text.lower().startswith(\"skill\"):\n",
    "        text = text[len(\"skill\"):].strip()\n",
    "    return text\n",
    "\n",
    "# Clean individual triples\n",
    "def clean_triple(triple):\n",
    "    subject, predicate, obj = triple\n",
    "    subject = clean_entity(subject)\n",
    "    obj = clean_entity(obj)\n",
    "    return (subject, predicate.strip(), obj)\n",
    "\n",
    "# Remove markdown code block markers if present\n",
    "def clean_text_block(text):\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    text = text.strip()\n",
    "\n",
    "    if text.startswith(\"```\") and text.endswith(\"```\"):\n",
    "        lines = text.splitlines()\n",
    "        # Remove first line (```python or ```) and last line (```)\n",
    "        text = \"\\n\".join(lines[1:-1]).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Parse the triples using ast and regex as fallback\n",
    "def parse_triples(text):\n",
    "    text = clean_text_block(text)\n",
    "\n",
    "    try:\n",
    "        triples = ast.literal_eval(text)\n",
    "        if isinstance(triples, list):\n",
    "            return [clean_triple(t) for t in triples if isinstance(t, tuple) and len(t) == 3]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Fallback — regex parsing\n",
    "    pattern = r'\\(\"(.+?)\",\\s*\"(.*?)\",\\s*\"(.*?)\"\\)'\n",
    "    matches = re.findall(pattern, text)\n",
    "\n",
    "    if matches:\n",
    "        return [clean_triple(t) for t in matches]\n",
    "\n",
    "    return []\n",
    "\n",
    "# Apply parsing to all entries\n",
    "parsed_triples = []\n",
    "\n",
    "for entry in extracted:\n",
    "    raw_output = entry.get('triples')\n",
    "    if raw_output and not entry.get('error'):\n",
    "        triples = parse_triples(raw_output)\n",
    "        if triples:\n",
    "            parsed_triples.extend(triples)\n",
    "        else:\n",
    "            print(f\"Failed to parse entry: {entry['input'].get('name', 'N/A')}\")\n",
    "\n",
    "print(f\"Parsed and cleaned {len(parsed_triples)} triples.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf913a3",
   "metadata": {},
   "source": [
    "### 12. Building the Knowledge Graph in NetworkX\n",
    "\n",
    "I start constructing the knowledge graph using **NetworkX**, a Python library for graph analysis.\n",
    "\n",
    "In this step:\n",
    "\n",
    "- Each unique entity becomes a **node**  \n",
    "- Each subject–predicate–object triple forms a **directed edge** between nodes, with the predicate as the **edge label**\n",
    "\n",
    "This allows for inspection of the graph structure, basic property computation (such as the number of nodes and edges), and preliminary exploration before deploying to a graph database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4ca1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 406\n",
      "Number of edges: 273\n",
      "Sample edges with relationships:\n",
      "Deep Crimson Parma -[is]-> standard round wooden shield\n",
      "red paint -[symbolizes]-> blood of warriors\n",
      "Wooden shields -[offer]-> relatively high magic absorption\n",
      "Wooden shields -[offer]-> high magic absorption\n",
      "Dagger -[used by]-> citizens of the Undead Commune\n",
      "inner blade -[lined with]-> fish hook-like barbs\n",
      "weapon -[had]-> ritualistic use\n",
      "Bellowing Dragoncrest Ring -[greatly boosts]-> sorceries\n",
      "Bellowing Dragoncrest Ring -[given to]-> those who are deemed fit to undertake the journey of discovery in Vinheim\n",
      "Bellowing dragon -[symbolizes]-> the true nature of the consummate sorcerer\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# 2. Add edges from triples\n",
    "for subj, rel, obj in parsed_triples:\n",
    "    G.add_node(subj)\n",
    "    G.add_node(obj)\n",
    "    G.add_edge(subj, obj, label=rel)\n",
    "\n",
    "# 3. Basic graph info\n",
    "print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {G.number_of_edges()}\")\n",
    "\n",
    "# 4. Inspect some edges\n",
    "print(\"Sample edges with relationships:\")\n",
    "for u, v, data in list(G.edges(data=True))[:10]:\n",
    "    print(f\"{u} -[{data['label']}]-> {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a0c8b6",
   "metadata": {},
   "source": [
    "### 13. Exporting the Graph to CSV\n",
    "\n",
    "To document the graph structure and maintain a clear, reproducible dataset, I export the graph to two CSV files:\n",
    "\n",
    "- A **nodes** file (`nodes.csv`), where each unique entity becomes a node  \n",
    "- An **edges** file (`edges.csv`), where each subject–predicate–object triple becomes a directed relationship\n",
    "\n",
    "These CSV files serve both as a backup of the structured graph and as an intermediate step for transparency. While the primary pipeline uploads the graph directly into Neo4j using Python (see the next section), having the CSVs available facilitates inspection, reproducibility, and potential reuse in other graph environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea4138d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported 406 nodes to 'nodes.csv'.\n",
      "Exported 273 edges to 'edges.csv'.\n"
     ]
    }
   ],
   "source": [
    "# 1. Export nodes\n",
    "nodes = pd.DataFrame({\n",
    "    \"id\": list(G.nodes),\n",
    "    \"label\": \"Entity\"  # Assigning a generic label to all nodes\n",
    "})\n",
    "\n",
    "nodes.to_csv(\"nodes.csv\", index=False, encoding=\"utf-8\")\n",
    "print(f\"Exported {len(nodes)} nodes to 'nodes.csv'.\")\n",
    "\n",
    "# 2. Export edges\n",
    "edges = pd.DataFrame([\n",
    "    {\n",
    "        \"source\": u,\n",
    "        \"target\": v,\n",
    "        \"type\": data[\"label\"]  # The relationship label\n",
    "    }\n",
    "    for u, v, data in G.edges(data=True)\n",
    "])\n",
    "\n",
    "edges.to_csv(\"edges.csv\", index=False, encoding=\"utf-8\")\n",
    "print(f\"Exported {len(edges)} edges to 'edges.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c5e2c9",
   "metadata": {},
   "source": [
    "### 14. Loading the Graph into Neo4j AuraDB with Python\n",
    "\n",
    "In this step, I connect to Neo4j AuraDB and upload the graph directly from the `nodes.csv` and `edges.csv` files.\n",
    "\n",
    "The process includes:\n",
    "\n",
    "- Establishing a secure connection to the Neo4j instance using the official Neo4j Python driver.\n",
    "- Creating nodes from the `nodes.csv` file, with label `Entity` and property `id`.\n",
    "- Creating relationships from the `edges.csv` file, using the `source` and `target` columns to define the start and end nodes, and the `type` column to define the relationship type.\n",
    "\n",
    "This automated approach directly populates the graph database with the structured knowledge extracted from the Dark Souls item descriptions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc661c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 406 nodes and 273 edges.\n",
      "Creating nodes...\n",
      "✅ Nodes created successfully.\n",
      "Creating relationships...\n",
      "✅ Relationships created successfully.\n",
      "✅ Graph loaded into Neo4j AuraDB successfully.\n"
     ]
    }
   ],
   "source": [
    "# 1. Load nodes and edges CSV\n",
    "nodes = pd.read_csv(\"nodes.csv\", dtype=str).fillna(\"\")\n",
    "edges = pd.read_csv(\"edges.csv\", dtype=str).fillna(\"\")\n",
    "\n",
    "print(f\"Loaded {len(nodes)} nodes and {len(edges)} edges.\")\n",
    "\n",
    "# 2. Configure Neo4j connection\n",
    "uri = \"neo4j+s://708e44c6.databases.neo4j.io\"\n",
    "username = \"neo4j\"\n",
    "password = \"TqOd26SToKqnpQn4LEhcWIDn0HXKeTV7GBSq74lziTQ\"\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "\n",
    "\n",
    "# 3. Function to create nodes\n",
    "def create_nodes(tx, nodes_df):\n",
    "    query = \"\"\"\n",
    "    UNWIND $rows AS row\n",
    "    MERGE (n:Entity {id: row.id})\n",
    "    \"\"\"\n",
    "    data = [{\"id\": row[\"id\"]} for _, row in nodes_df.iterrows()]\n",
    "    tx.run(query, rows=data)\n",
    "\n",
    "\n",
    "# 4. Function to create relationships\n",
    "def create_relationships(tx, edges_df):\n",
    "    query = \"\"\"\n",
    "    UNWIND $rows AS row\n",
    "    MATCH (a:Entity {id: row.source})\n",
    "    MATCH (b:Entity {id: row.target})\n",
    "    MERGE (a)-[r:RELATION {type: row.rel_type}]->(b)\n",
    "    SET r.label = row.rel_type\n",
    "    \"\"\"\n",
    "    data = [\n",
    "        {\n",
    "            \"source\": row[\"source\"],\n",
    "            \"target\": row[\"target\"],\n",
    "            \"rel_type\": row[\"type\"].upper().replace(\" \", \"_\")\n",
    "        }\n",
    "        for _, row in edges_df.iterrows()\n",
    "    ]\n",
    "    tx.run(query, rows=data)\n",
    "\n",
    "\n",
    "# 5. Push data to Neo4j\n",
    "with driver.session() as session:\n",
    "    print(\"Creating nodes...\")\n",
    "    session.execute_write(create_nodes, nodes)\n",
    "    print(\"✅ Nodes created successfully.\")\n",
    "\n",
    "    print(\"Creating relationships...\")\n",
    "    session.execute_write(create_relationships, edges)\n",
    "    print(\"✅ Relationships created successfully.\")\n",
    "\n",
    "driver.close()\n",
    "print(\"✅ Graph loaded into Neo4j AuraDB successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5784cffe",
   "metadata": {},
   "source": [
    "### 15. Building an Interactive Graph App with Streamlit\n",
    "\n",
    "To explore the knowledge graph interactively, I created a web application using Streamlit.\n",
    "\n",
    "The app connects directly to the Neo4j AuraDB instance, retrieves nodes and relationships using Cypher queries, and renders an interactive graph using PyVis. Users can visually navigate the graph, inspect connections, and explore the Dark Souls lore.\n",
    "\n",
    "This solution improves user experience by offering an intuitive and dynamic interface for graph exploration and discovery.\n",
    "\n",
    "🔗 **Try the app here:** [Dark Souls Knowledge Graph Explorer](https://your-streamlit-cloud-link)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6bafa9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lewagon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
